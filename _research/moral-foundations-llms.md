---
title: "Differences in the Moral Foundations of Large Language Models"
collection: research
permalink: /research/moral-foundations-llms
excerpt: "I conduct a series of experiments to compare the moral judgments of sixteen frontier language models from all major US model providers using Jonathan Haidt's Moral Foundations Theory. I find substantial evidence that differences in model judgments correspond to differences in moral foundations, large differences between model judgments and a human baseline, and clustering of model judgments by model provider."
slidesurl: '/files/MFT_LLMs_slides.pdf'
paperurl: '/files/MFT_LLMs.pdf'
repourl: 'https://github.com/peterkirgis/llm-moral-foundations'
citation: 'Kirgis, Peter. (2025). "Differences in the Moral Foundations of Large Language Models." Preprint.'
---

Large language models are increasingly being used in critical domains of politics, business, and education, but the nature of their normative ethical judgment remains opaque. Alignment research has, to date, not sufficiently utilized perspectives and insights from the field of moral psychology to inform training and evaluation of frontier models. I perform a series of synthetic experiments on a wide range of models from most major model providers using Jonathan Haidt's influential moral foundations theory (MFT) to elicit diverse value judgments from LLMs. I then use principal component analysis (PCA) to validate the construct of MFT in my LLM sample and explain variation in elicited moral judgment across models. My results suggest that models display different moral foundations from one another and from a nationally representative human baseline. This work seeks to spur further analysis of LLMs using MFT, including finetuning of open-source models, and greater deliberation by policymakers on the importance of moral foundations for LLM alignment.